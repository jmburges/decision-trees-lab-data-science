{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the last lesson, we learned how to \"Scrub\" a dataset and prepare it for predictive analytics tasks such as classification or regression.  In this lab, we'll prepare a dataset and go one step further, using **_Decision Trees_** to classify whether or not candy is chocolate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Complete a small data science project end-to-end using the OSEMN Data Science Process.  \n",
    "* Use holdout validation to split our dataset into _training_ and _testing_ sets, including Cross Validation\n",
    "* Demonstrate the relationship between Entropy and Information Gain, and use Entropy/GINI impurity to manually build a Decision Stump (Decision Tree with a single split)\n",
    "* Train a Decision Tree Classifier model from `scikit-learn` on our _training_ set, and visualize the model for easy interpretation\n",
    "* Use our trained model to make predictions on our _testing_ set, to evaluate how well our model will do on real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Data Science\n",
    "\n",
    "For this lab, we'll follow the steps of our now familiar Data Science Process, OSEMN.  Our goal is to build a Decision Tree that can look at the attributes of a piece of candy and accurately classify if the candy is chocolate or not. This is a **_binary classification_** problem--each prediction we need to make will either be yes (1) or no (0).\n",
    "\n",
    "Let's outline the steps we'll need to take in this lab:\n",
    "\n",
    "<center><h3>Obtain</h3></center>  \n",
    "We'll begin by reading the dataset into a `pandas` dataframe and inspecting a sample to get a look at our data.  \n",
    "\n",
    "<center><h3>Scrub</h3></center>  \n",
    "We'll store our target feature (the `'chocolate'` column) in a separate variable, so that we can remove it from the dataset later.  We'll also double-check that our dataset is in a format that will work for modeling, checking the data type of each column, and making sure we have no null values.  \n",
    "\n",
    "<center><h3>Explore</h3></center>   \n",
    "We'll briefly explore the model by examining and visualizing our two main groups of candy--chocolate and non-chocolate candies. We should always--**_always_** get a feel for the data we are working with before we jump into modeling.\n",
    "\n",
    "<center><h3>Modeling</h3></center>  \n",
    "This step will take up the majority of this lab, by far.  We'll start by building small, simple decision trees with only one or two splits by hand, just to get a feel for the concept of entropy and for how Decision Trees work. Once we have a decent inuition for the logic behind Decision Trees, we'll dive into Sci-Kit Learn and use its `DecisionTreeClassifier` class.  As we work through the modeling step, we'll also follow best practices by splitting our data into `training` and `testing` sets, to make sure that our model isn't overfitting.  \n",
    "\n",
    "<center><h3>iNterpret</h3></center>  \n",
    "Finally, we'll examine our newly trained model to see how well it did.  One handy feature of Decision Trees is that they're extremely easy to visualize.  In this step, we'll visualize our model and see how it helps us interpret our results.  \n",
    "\n",
    "<center><h3>Let's get started!</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining our data\n",
    "For this lab, we'll be working with 538's [Ultimate Halloween Candy Power Rankings Dataset](https://www.kaggle.com/fivethirtyeight/the-ultimate-halloween-candy-power-ranking). This dataset contains data on 85 different types of candy.  The dataframe contains a comphrensive list of attributes we can use to compare the candies.  The data is currently stored in a .csv file.  We'll use `pandas` to read the data in to a data frame.  \n",
    "\n",
    "For convenience's sake, we've reproduced the **_Data Dictionary_** from the link above.  Data Dictionaries are used to tell us exactly what everything means in a dataset, so that we can correctly interpret the data when we look at it.\n",
    "\n",
    "\n",
    "<center><h3>Data Dictionary</h3></center>\n",
    "\n",
    "| Feature          | Definition                                                   | Data Type               |\n",
    "|------------------|--------------------------------------------------------------|-------------------------|\n",
    "| chocolate        | Does it contain chocolate?                                   | Boolean (0=False, 1=True)                |\n",
    "| fruity           | Is it fruit flavored?                                        | Boolean (0=False, 1=True)                |\n",
    "| caramel          | Is there caramel in the candy?                               | Boolean (0=False, 1=True)                |\n",
    "| peanutalmondy    | Does it contain peanuts, peanut butter or almonds?           | Boolean (0=False, 1=True)                |\n",
    "| nougat           | Does it contain nougat?                                      | Boolean (0=False, 1=True)               |\n",
    "| crispedricewafer | Does it contain crisped rice, wafers, or a cookie component? | Boolean (0=False, 1=True)                |\n",
    "| hard             | Is it a hard candy?                                          | Boolean (0=False, 1=True)               |\n",
    "| bar              | Is it a candy bar?                                           | Boolean (0=False, 1=True)                |\n",
    "| pluribus         | Is it one of many candies in a bag or box?                   | Boolean (0=False, 1=True)                |\n",
    "| sugarpercent     | The percentile of sugar it falls under within the data set.  | Float 0.0 - 1.0 (Percentage) |\n",
    "| pricepercent     | The unit price percentile compared to the rest of the set.   | Float 0.0 - 1.0 (Percentage) |\n",
    "| winpercent       | The overall win percentage according to the 269,000 matchups | Float 00.0 - 100.0 (Percentage) |\n",
    "\n",
    "In the cell below, complete the following steps to import the dataset:\n",
    "\n",
    "1) Import `pandas` and alias it as `pd`\n",
    "\n",
    "2) Pass in `'candy-data.csv'` to `pd.read_csv()` to read the data into a dataframe.  Store the result in the `df` variable. \n",
    "\n",
    "3) Display the first few rows of the dataframe by using our dataframe object's `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in dataset: 85\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>competitorname</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>fruity</th>\n",
       "      <th>caramel</th>\n",
       "      <th>peanutyalmondy</th>\n",
       "      <th>nougat</th>\n",
       "      <th>crispedricewafer</th>\n",
       "      <th>hard</th>\n",
       "      <th>bar</th>\n",
       "      <th>pluribus</th>\n",
       "      <th>sugarpercent</th>\n",
       "      <th>pricepercent</th>\n",
       "      <th>winpercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100 Grand</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.860</td>\n",
       "      <td>66.971725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 Musketeers</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.511</td>\n",
       "      <td>67.602936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One dime</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.116</td>\n",
       "      <td>32.261086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One quarter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.511</td>\n",
       "      <td>46.116505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Air Heads</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.511</td>\n",
       "      <td>52.341465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  competitorname  chocolate  fruity  caramel  peanutyalmondy  nougat  \\\n",
       "0      100 Grand          1       0        1               0       0   \n",
       "1   3 Musketeers          1       0        0               0       1   \n",
       "2       One dime          0       0        0               0       0   \n",
       "3    One quarter          0       0        0               0       0   \n",
       "4      Air Heads          0       1        0               0       0   \n",
       "\n",
       "   crispedricewafer  hard  bar  pluribus  sugarpercent  pricepercent  \\\n",
       "0                 1     0    1         0         0.732         0.860   \n",
       "1                 0     0    1         0         0.604         0.511   \n",
       "2                 0     0    0         0         0.011         0.116   \n",
       "3                 0     0    0         0         0.011         0.511   \n",
       "4                 0     0    0         0         0.906         0.511   \n",
       "\n",
       "   winpercent  \n",
       "0   66.971725  \n",
       "1   67.602936  \n",
       "2   32.261086  \n",
       "3   46.116505  \n",
       "4   52.341465  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2\n",
    "df = pd.read_csv('candy-data.csv')\n",
    "\n",
    "# Step 3\n",
    "print(\"number of rows in dataset: {}\".format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrubbing our data\n",
    "\n",
    "Take a look at the example data displayed above.  What do you notice?\n",
    "\n",
    "We're going to use this dataset to predict which candies are chocolate based on the features contained above.  Before we can do that, we need to do a couple things to prepare our dataset for exploration and modeling:\n",
    "\n",
    "**_Store the labels in a separate variable_**. We need to store the `chocolate` column in a separate variable, so that we can remove it before our modeling step (we'll leave it in until we finish with the _Explore_ step to make things simpler).  The `chocolate` column will act as our **_labels_**, since the order they are in will still correspond to the order of the dataframe--for example, `target[0]` will still refer to the 100 grand bar, which is at index 0 in `df`.  \n",
    "\n",
    "**_Remove columns that aren't useful_**.  There's one column in the dataset that isn't useful for making predictions about the chocolatey-ness of candies.  Can you figure out which one? If you guessed `competitorname`, you are correct--give yourself a piece of candy.  This data isn't useful each name is unique to it's given row, and we can't use them to make predictions about other rows.  An easy way to understand this is to ask yourself if you could make predictions if the name was the only column you had to work with.  The answer is no--knowing if _100 Grand_ is chocolate or not gives you no information about _Air Heads_, and vice versa.  In this case, our best bet is to drop the column from the dataframe all together.  \n",
    "\n",
    "**_Make all percentages columns the same scale_**.  Take a look at the last three columns in the dataframe displayed above.  Notice what's wrong here?  Each column is meant to represent a percentage value between 0 and 100%, but the first two are scaled between 0 - 1, while the `winpercent` column is between 0 and 100.  This is the type of small mistake that is really easy to miss, but can cause us big problems down the road.  We'll avoid those problems by dividing the `winpercent` column by 100 to make it the same scale as `sugarpercent` and `pricepercent`.\n",
    "\n",
    "**_Check for null values_**.  In order for our model to correctly learn from the data, we need to make sure there are no **_null_** values in the dataset.  In `pandas`, null values are represented as `NaN`.  No matter how big or small the dataset is, we always have to detect and deal with null values.  Even if we have a billion rows of data, if 1 cell contains a null value, that's enough to cause an error.  Luckily, `pandas` comes built in with some really handy tools that makes checking for null values a cinch.  \n",
    "\n",
    "**_Double check the data type of each column_**.  This one may seem like a no-brainer, but it's an easy mistake to make.  When we look at the sample of the dataframe printed out above and see a number, there's no immediate way to tell if it's encoded as a string, integer, or float.  Before we move on to the next step, we'll want to write a few lines of code to double check that everything is, in fact, encoded as floats or ints, and not strings.  \n",
    "\n",
    "For this stage of the process, we'll need to complete the following steps:\n",
    "\n",
    "1) Store the `chocolate` column in the `target` variable.  Recall that in `pandas`, we can use the python list slicing syntax, but we pass in the column name instead of an index value.  (e.g. `df['caramel']` would retreive the 'caramel' column from the `df` object).  \n",
    "\n",
    "2) call `pd.drop()`, and pass in the string `'competitorname'` as the value.  Also pass in the keyword parameters `axis=1` to tell the function that the item we passed in corresponds to a column, not a row.  We'll store the result that is returned in `df`.\n",
    "\n",
    "3) Set the dataframe's `winpercent` column to `winpercent` divided by 100.0. \n",
    "\n",
    "4) To check for null values in each column, call `df.isna().any()`. \n",
    "\n",
    "5) Check the `df` object's `.dtype` attribute to see what data type each column is encoded as.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>fruity</th>\n",
       "      <th>caramel</th>\n",
       "      <th>peanutyalmondy</th>\n",
       "      <th>nougat</th>\n",
       "      <th>crispedricewafer</th>\n",
       "      <th>hard</th>\n",
       "      <th>bar</th>\n",
       "      <th>pluribus</th>\n",
       "      <th>sugarpercent</th>\n",
       "      <th>pricepercent</th>\n",
       "      <th>winpercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.669717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.676029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.322611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.461165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.523415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  fruity  caramel  peanutyalmondy  nougat  crispedricewafer  hard  \\\n",
       "0          1       0        1               0       0                 1     0   \n",
       "1          1       0        0               0       1                 0     0   \n",
       "2          0       0        0               0       0                 0     0   \n",
       "3          0       0        0               0       0                 0     0   \n",
       "4          0       1        0               0       0                 0     0   \n",
       "\n",
       "   bar  pluribus  sugarpercent  pricepercent  winpercent  \n",
       "0    1         0         0.732         0.860    0.669717  \n",
       "1    1         0         0.604         0.511    0.676029  \n",
       "2    0         0         0.011         0.116    0.322611  \n",
       "3    0         0         0.011         0.511    0.461165  \n",
       "4    0         0         0.906         0.511    0.523415  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if columns contain NaNs:\n",
      "chocolate           False\n",
      "fruity              False\n",
      "caramel             False\n",
      "peanutyalmondy      False\n",
      "nougat              False\n",
      "crispedricewafer    False\n",
      "hard                False\n",
      "bar                 False\n",
      "pluribus            False\n",
      "sugarpercent        False\n",
      "pricepercent        False\n",
      "winpercent          False\n",
      "dtype: bool\n",
      "Data type of each column:\n",
      "chocolate             int64\n",
      "fruity                int64\n",
      "caramel               int64\n",
      "peanutyalmondy        int64\n",
      "nougat                int64\n",
      "crispedricewafer      int64\n",
      "hard                  int64\n",
      "bar                   int64\n",
      "pluribus              int64\n",
      "sugarpercent        float64\n",
      "pricepercent        float64\n",
      "winpercent          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "target = df['chocolate']\n",
    "\n",
    "# Step 2\n",
    "df = df.drop('competitorname', axis=1)\n",
    "\n",
    "# Step 3\n",
    "df['winpercent'] = df['winpercent'] / 100\n",
    "display(df.head())\n",
    "# Step 5\n",
    "print(\"Check if columns contain NaNs:\")\n",
    "print(df.isna().any())\n",
    "\n",
    "# Step 5\n",
    "print(\"Data type of each column:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "During the Explore step, we want to get a feel for our data, and see if we can spot anything that might help us during the modeling step.  Luckily, our goal is pretty clear here, and the features in our dataset our self-explanatory.  This makes this step pretty easy for us.  \n",
    "\n",
    "For this stage of the process, we'll need to complete the following steps:\n",
    "\n",
    "1. Use our dataframe's `.groupby()` method to get the total number of instances for each feature for candy's that are and aren't chocolate.  (Hint: Once you've called `.groupby()` on the `chocolate` column, chain it with the `.sum()` method).  If you're unsure of how to do this, see the syntax hint below.\n",
    "\n",
    "```python\n",
    "# If we wanted to see the sum of features for caramel/non-caramel groupings:\n",
    "grouped_by_caramel = df.groupby('caramel').sum()\n",
    "```\n",
    "<br>\n",
    "1. Visualize the results in a histogram using the dataframe's `.plot().bar()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruity</th>\n",
       "      <th>caramel</th>\n",
       "      <th>peanutyalmondy</th>\n",
       "      <th>nougat</th>\n",
       "      <th>crispedricewafer</th>\n",
       "      <th>hard</th>\n",
       "      <th>bar</th>\n",
       "      <th>pluribus</th>\n",
       "      <th>sugarpercent</th>\n",
       "      <th>pricepercent</th>\n",
       "      <th>winpercent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>21.741</td>\n",
       "      <td>16.465</td>\n",
       "      <td>20.228283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>18.944</td>\n",
       "      <td>23.390</td>\n",
       "      <td>22.540966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fruity  caramel  peanutyalmondy  nougat  crispedricewafer  hard  \\\n",
       "chocolate                                                                    \n",
       "0              37        4               2       1                 0    14   \n",
       "1               1       10              12       6                 7     1   \n",
       "\n",
       "           bar  pluribus  sugarpercent  pricepercent  winpercent  \n",
       "chocolate                                                         \n",
       "0            1        32        21.741        16.465   20.228283  \n",
       "1           20        12        18.944        23.390   22.540966  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_by_chocolate = df.groupby('chocolate').sum()\n",
    "grouped_by_chocolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11138b320>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_by_chocolate.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's something not quite right about that graph. Take a minute to examine the graph and the dataframe above and see if you can figure out what's wrong...\n",
    "\n",
    "\n",
    "### Dealing with Different Data Types\n",
    "If you noticed the percentage columns, you're right! Using the `.sum()` function as an aggregator to group by makes sense for any of the boolean columns that contain only `0` or `1`, since interpreting this is intiutive.  For instance, we can look at the `grouped_by_chocolate` dataframe and notice that 37 different non-chocolate candies are fruit-flavored.  However, this strategy breaks down for the last three columns in the dataframe--`sugarpercent`, `pricepercent`, and `winpercent`.  Although the sum of these percentages still gives us a little bit of insight (e.g. chocolate candies won more head-to-head matchups overall than non-chocolate), these massive numbers don't tell us as much as averages would--they also break our visualization!\n",
    "\n",
    "We'll fix this problem by doing two things:\n",
    "\n",
    "1.  Dropping `sugarpercent`, `pricepercent`, and `winpercent` columns from our `grouped_by_chocolate` dataframe, and running the bar chart visualization again. Recall that you can drop columns from a dataframe by a list of column names to the dataframe's `.drop()` method.  Also pass the parameters `axis=1` and `inplace=False`.  Store the dataframe object returned in the `bools_only_df` variable.  Then, visualize `bools_only_df` as a bar chart. \n",
    "\n",
    "1.  Create a grouped dataframe called `avg_percentages` that consists of just the average percentages for our 3 percentage columns (`sugarpercent`, `pricepercent`, and `winpercent`) for both chocolate and non-chocolate candies.  We'll display this dataframe, as well as a visualization for each. (This will take three steps--see the syntax hint below!)\n",
    "\n",
    "```Python\n",
    "# Step 1: Put all the columns we want in our new dataframe in a list\n",
    "cols = ['chocolate', 'sugarpercent', 'pricepercent', 'winpercent']\n",
    "# Step 2: Slice the 'df' dataframe and pass the cols list as our index\n",
    "percent_df = df[cols]\n",
    "# Step 3: Use .groupby() on the 'chocolate' column as you did above, and chain it with a .mean() method--see if you can figure this one out for yourself!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1149e8160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1149d5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1.1: Drop ['sugarpercent', 'pricepercent', 'winpercent'] from grouped_by_chocolate.\n",
    "bools_only_df = grouped_by_chocolate.drop(['sugarpercent', 'pricepercent', 'winpercent'], axis=1, inplace=False)\n",
    "\n",
    "# Step 1.2: Visualize bools_only_df as a bar chart by calling '.plot.bar()' on it. \n",
    "bools_only_df.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sugarpercent</th>\n",
       "      <th>pricepercent</th>\n",
       "      <th>winpercent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.452937</td>\n",
       "      <td>0.343021</td>\n",
       "      <td>0.421423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.632162</td>\n",
       "      <td>0.609215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sugarpercent  pricepercent  winpercent\n",
       "chocolate                                        \n",
       "0              0.452937      0.343021    0.421423\n",
       "1              0.512000      0.632162    0.609215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x114a685f8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1149ef860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2.1: Store the names of the columns we want in this dataframe in a list\n",
    "cols = ['chocolate', 'sugarpercent', 'pricepercent', 'winpercent']\n",
    "\n",
    "# Step 2.2: Slice the df object.  Pass the cols variable as the index for the slicing operation.\n",
    "percent_df = df[cols]\n",
    "\n",
    "# Step 2.3 Group by the chocolate column as we did above.  This time, chain it with a '.mean()' \n",
    "# call instead of '.sum().\n",
    "avg_percentages = percent_df.groupby('chocolate').mean()\n",
    "display(avg_percentages)\n",
    "\n",
    "# Step 2.4: Plot avg_percentages as a bar chart, as we did above.\n",
    "avg_percentages.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for some quick Exploratory Data Analysis!  The great thing about visualizing during the _Explore_ step is that important information becomes obvious.  For instance, take a look at the _fruity_ columns in our bar chart above.  It seems like an overwhelming amount of the time when a candy is fruity, it's also not chocolate.  This is the type of information that we can use to help build our classifier during the next step!\n",
    "\n",
    "Based on our explorations above, can you find two more things make it easy to tell the difference between chocolate and non-chocolate candies?  Double-click on this text cell and write them down below--we're going to use them in the next section!\n",
    "\n",
    "**_Answer 1:_**  Fruity candies are almost always non-chocolate.  \n",
    "\n",
    "**_Answer 2:_**\n",
    "\n",
    "**_Answer 3:_**\n",
    "\n",
    "## Building our Model\n",
    "\n",
    "For this step, we'll use one of the most powerful tools in analytics--a **_Decision Tree_**.  Decision Trees are a type of predictive model that can be used for classification or regression.  Better yet, they are simple to interpret, and intuitively easy to understand how they work.  To get a feel for how Decision Trees work, we're going to build simple version of one on our own, before moving on to professional tools such as the **_scikit-learn_** library (called _sklearn_ for short).  \n",
    "\n",
    "#### Growing our own Decision Tree\n",
    "\n",
    "Let's do a thought experiment real quick. If we could only look at one feature before classifying each candy as chocolate or non-chocolate, and we our goal is to build the most accurate model possible--which feature would you look at?  Take a minute to think about this (Hint: review our findings during the _Explore_ step!)\n",
    "\n",
    "<center><img src='thinking.gif'></center>\n",
    "    \n",
    "Based on our Exploratory Data Analysis, we would get the best results from splitting on the `fruity` column!  It's accuracy won't be great, but mathematically, it would do better than any other single split we could make.  This sort of split is perfect for a basic `if/else` block in python.  Let's think about how this would look in Python.  \n",
    "\n",
    "```python\n",
    "def predict(row):\n",
    "    if row['fruity'] == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "```\n",
    "\n",
    "That's some pretty basic python code, but it's also something else--the most basic Decision Tree we can build--a **_Stump_**! (No, seriously.  Decision Trees with only one split are called a Stump).  \n",
    "\n",
    "We'll use this logic examine every piece of candy in the data set. Anytime we see that a candy is fruity, we'll immediately predict a `0`, and a `1` anytime that is isn't fruity.  This is pretty easy to code up--we just need to loop through the `fruity` column, and make a prediction for each item.  We'll store all our predictions in a list called `predictions`.\n",
    "\n",
    "To build our own little Decision Tree that could, complete the following steps:\n",
    "\n",
    "1. Slice the `fruity` column from the `df` dataframe.  Store it in the `fruity_data` variable.  \n",
    "<br>\n",
    "2. Complete the `make_predictions` function. This function will take in the `fruity_data` list, and loop through it.  Every time it sees a `1`, it should append a `0` to a list variable inside the function called `predictions`.  Every time it sees a `0` inside `fruity_data`, it should append a `1` to predictions.  After the loop is finished, return the `predictions` list.    \n",
    "<br>\n",
    "3. Call the `make_predictions` function and pass in the `fruity_data` variable.  Store the results in a variable called `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the fruity column from the `df` object.\n",
    "fruity_data = df['fruity']\n",
    "\n",
    "# Step 2: Complete the function.\n",
    "def make_predictions(fruity_data):\n",
    "    predictions = []\n",
    "    for i in fruity_data:\n",
    "        if i == 1:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(1)\n",
    "    return predictions\n",
    "\n",
    "# Step 3: Call function and store output in predictions variable\n",
    "predictions = make_predictions(fruity_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have predictions from our Decision Stump (take a second to groan at the name, if you must), let's write a function to see how well it did.  In order to measure the model's performance, we'll use the **_Accuracy_**  metric.  \n",
    "\n",
    "In order to understand accuracy, we need to think about the 4 potential cases can happen when we compare our predictions with the actual grouund truth (the labels that tell us if a candy actually is or isn't chocolate). \n",
    "\n",
    "These are: \n",
    "\n",
    "**_True Positive_**: We predicted it was chocolate, and it was. \n",
    "\n",
    "**_True Negative_**: We predicted it wasn't chocolate, and it wasn't.\n",
    "\n",
    "**_False Positive_**: We predicted that it was chocolate, but it wasn't.\n",
    "\n",
    "**_False Negative_**: We predicted that it wasn't chocolate, but it was.  \n",
    "\n",
    "Often, data scientists will place all of these in a table called a **_Confusion Matrix_**.  Take a look at this handy example pulled from [mlxtend](https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/):\n",
    "\n",
    "<center><img src='confusion_matrix.png'></center>\n",
    "\n",
    "We can use a confusion matrix to compute our **_accuracy_**.  Accuracy is defined as:  \n",
    "<br>  \n",
    "<center>\n",
    "$$\n",
    "\\normalsize \n",
    "\\frac{True\\ Positives + True\\ Negatives}{True\\ Positives + True\\ Negatives + False\\ Positives + False\\ Negatives}\n",
    "$$  \n",
    "</center>\n",
    "\n",
    "This is just a fancy way of saying we divide the number of predictions we got correct by the total number of predictions.  \n",
    "\n",
    "Next, we'll:\n",
    "\n",
    "1. Write a function called `accuracy` that takes in two parameters--`predictions` and `labels`.  This function should contain a variable that called `num_correct`, which should initially be set to `0`.  It should iterate through the lists, comparing each value in `predictions` with the value at the corresponding index in `labels`.  When they are equal, increment `num_correct` by 1.  Otherwise, do nothing.  Once the loop has finished, divide `num_correct` by the total number of items in the dataset and return that value.  This is our accuracy score.  \n",
    "\n",
    "2. Call the accuracy function by passing in our `predictions` variable and our `target` variable (we created this in the beginning of the lab).  Store the results in a variable called `stump_accuracy.` Run the cell to see the accuracy for our stump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Fruity Decision Stump: 0.8470588235294118\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    num_correct = 0\n",
    "    for i in range(len(predictions) - 1):\n",
    "        if predictions[i] == labels[i]:\n",
    "            num_correct += 1\n",
    "    \n",
    "    return num_correct / len(predictions)\n",
    "\n",
    "stump_accuracy = accuracy(predictions, target)\n",
    "print(\"Accuracy Score for Fruity Decision Stump: {}\".format(stump_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**_Whoa--84.7%!_** Not bad for a Stump. \n",
    "\n",
    "<img src='treestump.jpg' height=60% width=60%>\n",
    "_The wise and mighty Decision Stump in its natural habitat, 2018._\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "If we were to visualize what our stump looked like as a Decision Tree, it would look like this: \n",
    "\n",
    "<img src='stump_viz.png' height=40% width=40%></center>\n",
    "\n",
    "Usually, one split isn't enough to do so well.  What if, instead of immediately labeling each sub-group after the first split as chocolate/non-chocolate, we repeated our splitting process, searching each sub-group for a split that would more accurately help us split up chocolate/non-chocolate candies?  This is exactly how Decision Trees work!\n",
    "\n",
    "\n",
    "### How Decision Trees Work\n",
    "\n",
    "This brings us to the main idea behind Decision Trees.  With every split, a Decision Tree tries to maximize the the amount of **_information gain_** with each split.  Think about why we picked `fruity` as the best choice for our split.  By splitting based on the value of this column, we split our dataset into two groups with the most chocolates on one side and non-chocolates on the other.  Conversely, if we chose to split on a column where both sub-groups were equally composed of chocolate and non-chocolate candies, we would have gained no information at all--our model after the split is no better at separating the two types after the split than before it.  \n",
    "\n",
    "When things are messy and the targets are mixed up, we say that it has high **_entropy_** (you may have noticed the visualization above uses _gini_ instead of entropy--for our purposes, we can treat them as essentially the same thing).  Entropy is a measure of disorder.  A sample where every item is the same has zero entropy, whereas a sample that has an equal number of 2 (or more) classes in it has maximum entropy. \n",
    "\n",
    "In a professional-quality Decision Tree algorithm, such as the one we'll use from the _sklearn_ library, the algorithm examines every possible rule it could use to make a split, and picks the one that will decrease entropy the most.  It continues this until it has accurately classified everything (unlikely), or until it has reached a predetermined threshold such as the number of splits.  \n",
    "\n",
    "\n",
    "### Using Professional Modeling Tools to Build a Decision Tree\n",
    "\n",
    "This brings us to the most fun part of the lab--building models using the Data Science community's favorite Machine Learning library, _sklearn_! The remaining parts of this stage will focus on creating our model using _sklearn_, and on validating our model with a **_testing set_** to double check that the Decision Tree that is built generalizes well to real-world data, and didn't just memorize the data in the training set we gave it (more on this later).\n",
    "\n",
    "Importing the things we need from sklearn is fairly straight-forward.  However, you're still encouraged to check out [sklearn's documentation for Decision Tree Classifiers!](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "We'll start by running the cell below to import what we need.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy.  Next, we'll create a version of the dataset that has the `chocolate` column removed.  This is the reason we stored the labels in our `chocolate` column in a variable called `target` at the beginning of the lab.  If we don't remove that column from the dataset before feeding the data to our Decision Tree Classifier, it will just treat that as another column.  It's a bit like having it take a test on the answer key--it won't actually learn anything, because the answers are already provided.  \n",
    "\n",
    "1.  Drop the `chocolate` column from the df object.  Also pass the parameters `axis=1` and `inplace=False`.  Store the dataframe that is returned in the `labels_dropped_df` variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dropped_df = df.drop('chocolate', axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  Now we have a dataset that contains everything but our labels, and a variable containing just our labels. Next, we'll randomly grab a sample of data (and their corresponding labels) to hold out as our **_test set_**.  Before we do this, let's quickly examine why this is a good idea.\n",
    "\n",
    "### Holdout Validation Methods, and Detecting Overfitting/Underfitting\n",
    "\n",
    "A cardinal rule of the Modeling step is that any models we learn should be **_generalizable_** to the real-world.  If they only apply to data in datasets they've already seen, then they aren't very useful.  Our goal is to build a model (in this case, a Decision Tree Classifier) that can look at the data make accurate predictions about the chocolatey-ness of other candies not in this dataaset.  However, our Decision Tree is built in such a way that its goal is to maximize performance on **_training data_** it's given.  Sometimes, we may think our models are getting high accuracy because they have successfully learned some great things that help it make predictions about candy, but in reality, the model has just memorized some of the specifics.  This means that whatever it learns won't apply to the real world, because it hasn't memorized the data it will see in the real world, only the data in the training set.  We call this **_Overfitting_**.  \n",
    "\n",
    "This is more intuitive when we think about how we learn, too.  Let's say you have a big test in Biology coming up soon, so you and your best friend buy a study guides with practice questions in them.  You study the material in the study guide, and try to learn everything you can about Biology for this test.  When you take the practice exam, you score a respectable 88%. \n",
    "\n",
    "Your friend, however, takes a different route.  Instead, they memorize every question in the study guide, and the corresponding answer to it.  When they take the practice test, they get a 100%.  \n",
    "\n",
    "**_Who is going to do better on the actual biology test? _**\n",
    "\n",
    "You are, because you learned a model of biology that generalizes well to the real world.  Although your friend may have walked into the test with high confidence, they were doomed to failure from the moment they sat down--after all, the practice test isn't the test.  It's going to have different questions on it.  \n",
    "\n",
    "The problem with modeling is when we give it the training data, all we ever see are the \"scores from the practice test\".  This may signal that the model learned what it needed to learn to do well on its given task in the real-world, but it may also have just memorized some of the data points to score higher (because getting a high score is it's goal). \n",
    "\n",
    "The easiest way to check how well our model will do in the real world is to cut off some of our training data and save it away for after the model is trained.  This data will become our **_testing set_**.  Since we have labels for it as well, we can ask the model to make predictions on the testing data, and then grade its performance to see how well it did.  This will be more similar to data seen in the real world, because we're asking the model to classify data it hasn't seen before.\n",
    "\n",
    "### Picking the right amount of data\n",
    "\n",
    "It's important that we pick enough data into our testing set so that we get a representative sample of our model's performance, but not so much that we take away too much training data and the model can't learn well.  \n",
    "\n",
    "By default, _sklearn_'s `train_test_split` method randomly grabs 25% of the total data in the dataset and places it in the testing set.  However, we can dial this number up or down as you see fit--for a full explanation, take a look at the [train_test_split documentation.](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "Run the cell below to randomly split our data and labels into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(labels_dropped_df, target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a convention, the datasets are always denoted with an `X` (capital because it's a 2-D matrix of data), and labels are always denoted with a `y` (lowercase because they are a 1-D array).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 1.0\n",
      "Score on testing set: 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Instantiate a DecisionTreeClassifier() object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Step 2: Call the clf object's .fit() method.  Pass in X_train as the first parameter, and y_train as the second. \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Use the clf object's .score() method to see how it does on the testing set.  Pass in X_test as the first \n",
    "# parameter, and y_test as the second. \n",
    "training_score = clf.score(X_train, y_train)\n",
    "testing_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Score on training set: {}\".format(training_score))\n",
    "print(\"Score on testing set: {}\".format(testing_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_90.9% on testing--that's great!_**\n",
    "\n",
    "Note that if you run the two cells above again, the testing score will likely change. This is because of the random sampling that occurs when we re-run the cell containing `train_test_split()`.  Sometimes, the testing sample will randomly contain some of the harder examples, while sometimes, it will contain some of the easier samples.  This is the reason why we see the scores fluctuate.  When the score fluctuates, that doesn't necessarily mean the model that specific time is better or worse--it just means that the composition of the training and testing sets is different.  \n",
    "\n",
    "If you really want to get a feel for how accurate the model is doing, we can run it multiple times and average all the scores together.  That's exactly where K-fold Cross Validation comes in. In K-fold cross validation, we choose \\[K\\] splits (5 is usually a good choice), and divide the data into \\[K\\] equal portions.  The first time we train a model, we use the first portion as our testing set, and combine every other potion to be our training set.  The second time, we use the second portion as our testing set, and combine every other portion to be our training set, and so on and so on, until every portion has had one instance where it is the testing set.  \n",
    "\n",
    "Since each of those models will have different scores, we can take the average of all of them to get a better feel for how well our model actually does.   \n",
    "\n",
    "(Note that when we use cross validation, we give it the full dataset and labels, instead of the output of `train_test_split()`, because cross-validation does it's own train-test splitting during the process!)\n",
    "\n",
    "Run the cell below to use get the **_cross-validation score_** for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Cross-Validation: [ 0.77777778  1.          0.76470588  0.8125      0.8125    ]\n",
      "Average score from Cross-Validation: 0.8334967320261437\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "cv_score = cross_val_score(clf, labels_dropped_df, target, cv=5)\n",
    "average_score = np.mean(cv_score)\n",
    "print(\"Results from Cross-Validation: {}\".format(cv_score))\n",
    "print(\"Average score from Cross-Validation: {}\".format(average_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good thing we used cross-validation.  If we had only run it once and got a 72.2% or 94.4%, our understanding of our model's performance would be off in both cases by more than 10%! \n",
    "\n",
    "## Interpreting our Results\n",
    "\n",
    "This brings us to the final stage of our Data Science Process--**_iNterpret_**.  During this stage, we would apply what we've learned from all of the stages so far to draw conclusions that can help us with our business goal.  This stage is often specific to the business use case, but there are some questions that we can ask that are fairly universal to this stage, regardless of the problem. \n",
    "\n",
    "The first and most pressing interpretation question we can ask is: is 81% accuracy good enough?\n",
    "\n",
    "The answer depends on your goals.  In some cases, any accuracy slightly better than flipping a coin is exellent (e.g. what if a model could improve Amazon's revenue by even a fraction of a percent?).  In other cases, an accuracy as 99% still might not be good enough (think self-driving cars, or other modeling examples where human life is on the line if our model isn't good enough).  The answer to this question is up to you to interpret, depending on the question you/your team is trying to answer.  \n",
    "\n",
    "Although we can't visualize every type of model, since we used a Decision Tree, we can in this case.  We'll end this session by visualizing our Decision Tree Classifier, so that we have a human-readable version that we can take to our boss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"682pt\" height=\"909pt\"\n",
       " viewBox=\"0.00 0.00 682.00 909.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 905)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-905 678,-905 678,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.200000\" stroke=\"#000000\" d=\"M414,-901C414,-901 341,-901 341,-901 335,-901 329,-895 329,-889 329,-889 329,-830 329,-830 329,-824 335,-818 341,-818 341,-818 414,-818 414,-818 420,-818 426,-824 426,-830 426,-830 426,-889 426,-889 426,-895 420,-901 414,-901\"/>\n",
       "<text text-anchor=\"start\" x=\"353\" y=\"-885.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">fruity ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"350\" y=\"-870.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.494</text>\n",
       "<text text-anchor=\"start\" x=\"346\" y=\"-855.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 63</text>\n",
       "<text text-anchor=\"start\" x=\"341\" y=\"-840.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [35, 28]</text>\n",
       "<text text-anchor=\"start\" x=\"337\" y=\"-825.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.701961\" stroke=\"#000000\" d=\"M359.5,-782C359.5,-782 265.5,-782 265.5,-782 259.5,-782 253.5,-776 253.5,-770 253.5,-770 253.5,-711 253.5,-711 253.5,-705 259.5,-699 265.5,-699 265.5,-699 359.5,-699 359.5,-699 365.5,-699 371.5,-705 371.5,-711 371.5,-711 371.5,-770 371.5,-770 371.5,-776 365.5,-782 359.5,-782\"/>\n",
       "<text text-anchor=\"start\" x=\"268\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">winpercent ≤ 0.471</text>\n",
       "<text text-anchor=\"start\" x=\"285\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.353</text>\n",
       "<text text-anchor=\"start\" x=\"281\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 35</text>\n",
       "<text text-anchor=\"start\" x=\"279\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [8, 27]</text>\n",
       "<text text-anchor=\"start\" x=\"261.5\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = non&#45;chocolate</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M354.7662,-817.8796C350.0435,-809.2335 345.0176,-800.0322 340.141,-791.1042\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"343.1963,-789.3964 335.3309,-782.2981 337.053,-792.752 343.1963,-789.3964\"/>\n",
       "<text text-anchor=\"middle\" x=\"328.2908\" y=\"-802.5864\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.964706\" stroke=\"#000000\" d=\"M483,-782C483,-782 402,-782 402,-782 396,-782 390,-776 390,-770 390,-770 390,-711 390,-711 390,-705 396,-699 402,-699 402,-699 483,-699 483,-699 489,-699 495,-705 495,-711 495,-711 495,-770 495,-770 495,-776 489,-782 483,-782\"/>\n",
       "<text text-anchor=\"start\" x=\"398\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">winpercent ≤ 0.481</text>\n",
       "<text text-anchor=\"start\" x=\"415\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.069</text>\n",
       "<text text-anchor=\"start\" x=\"411\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 28</text>\n",
       "<text text-anchor=\"start\" x=\"409\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [27, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"402\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>0&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M400.2338,-817.8796C404.9565,-809.2335 409.9824,-800.0322 414.859,-791.1042\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"417.947,-792.752 419.6691,-782.2981 411.8037,-789.3964 417.947,-792.752\"/>\n",
       "<text text-anchor=\"middle\" x=\"426.7092\" y=\"-802.5864\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.498039\" stroke=\"#000000\" d=\"M224,-663C224,-663 143,-663 143,-663 137,-663 131,-657 131,-651 131,-651 131,-592 131,-592 131,-586 137,-580 143,-580 143,-580 224,-580 224,-580 230,-580 236,-586 236,-592 236,-592 236,-651 236,-651 236,-657 230,-663 224,-663\"/>\n",
       "<text text-anchor=\"start\" x=\"139\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">winpercent ≤ 0.344</text>\n",
       "<text text-anchor=\"start\" x=\"156\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 12</text>\n",
       "<text text-anchor=\"start\" x=\"153\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [8, 4]</text>\n",
       "<text text-anchor=\"start\" x=\"143\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M267.3821,-698.8796C257.326,-689.6031 246.577,-679.6874 236.2446,-670.1559\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.5339,-667.506 228.8105,-663.2981 233.7876,-672.6511 238.5339,-667.506\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#399de5\" stroke=\"#000000\" d=\"M360.5,-655.5C360.5,-655.5 266.5,-655.5 266.5,-655.5 260.5,-655.5 254.5,-649.5 254.5,-643.5 254.5,-643.5 254.5,-599.5 254.5,-599.5 254.5,-593.5 260.5,-587.5 266.5,-587.5 266.5,-587.5 360.5,-587.5 360.5,-587.5 366.5,-587.5 372.5,-593.5 372.5,-599.5 372.5,-599.5 372.5,-643.5 372.5,-643.5 372.5,-649.5 366.5,-655.5 360.5,-655.5\"/>\n",
       "<text text-anchor=\"start\" x=\"292\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"282\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 23</text>\n",
       "<text text-anchor=\"start\" x=\"280\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 23]</text>\n",
       "<text text-anchor=\"start\" x=\"262.5\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = non&#45;chocolate</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>1&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M312.8498,-698.8796C312.9394,-688.2134 313.0361,-676.7021 313.1269,-665.9015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"316.6274,-665.844 313.2116,-655.8149 309.6276,-665.7851 316.6274,-665.844\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M213,-536.5C213,-536.5 140,-536.5 140,-536.5 134,-536.5 128,-530.5 128,-524.5 128,-524.5 128,-480.5 128,-480.5 128,-474.5 134,-468.5 140,-468.5 140,-468.5 213,-468.5 213,-468.5 219,-468.5 225,-474.5 225,-480.5 225,-480.5 225,-524.5 225,-524.5 225,-530.5 219,-536.5 213,-536.5\"/>\n",
       "<text text-anchor=\"start\" x=\"155\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"148\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4</text>\n",
       "<text text-anchor=\"start\" x=\"146\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [4, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"136\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M181.0517,-579.8796C180.4243,-569.2134 179.7472,-557.7021 179.1119,-546.9015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"182.5998,-546.5921 178.5185,-536.8149 175.6119,-547.0032 182.5998,-546.5921\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"transparent\" stroke=\"#000000\" d=\"M351.5,-544C351.5,-544 255.5,-544 255.5,-544 249.5,-544 243.5,-538 243.5,-532 243.5,-532 243.5,-473 243.5,-473 243.5,-467 249.5,-461 255.5,-461 255.5,-461 351.5,-461 351.5,-461 357.5,-461 363.5,-467 363.5,-473 363.5,-473 363.5,-532 363.5,-532 363.5,-538 357.5,-544 351.5,-544\"/>\n",
       "<text text-anchor=\"start\" x=\"251.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">peanutyalmondy ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"282\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"275\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8</text>\n",
       "<text text-anchor=\"start\" x=\"273\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [4, 4]</text>\n",
       "<text text-anchor=\"start\" x=\"263\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M225.4702,-579.8796C234.7338,-570.6931 244.6296,-560.8798 254.1552,-551.4336\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"256.7146,-553.8247 261.3507,-544.2981 251.7856,-548.8543 256.7146,-553.8247\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.250980\" stroke=\"#000000\" d=\"M287.5,-425C287.5,-425 193.5,-425 193.5,-425 187.5,-425 181.5,-419 181.5,-413 181.5,-413 181.5,-354 181.5,-354 181.5,-348 187.5,-342 193.5,-342 193.5,-342 287.5,-342 287.5,-342 293.5,-342 299.5,-348 299.5,-354 299.5,-354 299.5,-413 299.5,-413 299.5,-419 293.5,-425 287.5,-425\"/>\n",
       "<text text-anchor=\"start\" x=\"192.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">pricepercent ≤ 0.418</text>\n",
       "<text text-anchor=\"start\" x=\"216\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.49</text>\n",
       "<text text-anchor=\"start\" x=\"212\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 7</text>\n",
       "<text text-anchor=\"start\" x=\"210\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [3, 4]</text>\n",
       "<text text-anchor=\"start\" x=\"189.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = non&#45;chocolate</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M281.4657,-460.8796C276.936,-452.3236 272.1185,-443.2238 267.4382,-434.3833\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"270.4006,-432.4983 262.6284,-425.2981 264.2141,-435.7736 270.4006,-432.4983\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M403,-417.5C403,-417.5 330,-417.5 330,-417.5 324,-417.5 318,-411.5 318,-405.5 318,-405.5 318,-361.5 318,-361.5 318,-355.5 324,-349.5 330,-349.5 330,-349.5 403,-349.5 403,-349.5 409,-349.5 415,-355.5 415,-361.5 415,-361.5 415,-405.5 415,-405.5 415,-411.5 409,-417.5 403,-417.5\"/>\n",
       "<text text-anchor=\"start\" x=\"345\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"338\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"326\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>4&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M325.5343,-460.8796C331.3558,-449.8835 337.6527,-437.9893 343.5227,-426.9015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"346.7476,-428.2905 348.3333,-417.8149 340.5611,-425.0152 346.7476,-428.2905\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.333333\" stroke=\"#000000\" d=\"M218.5,-306C218.5,-306 130.5,-306 130.5,-306 124.5,-306 118.5,-300 118.5,-294 118.5,-294 118.5,-235 118.5,-235 118.5,-229 124.5,-223 130.5,-223 130.5,-223 218.5,-223 218.5,-223 224.5,-223 230.5,-229 230.5,-235 230.5,-235 230.5,-294 230.5,-294 230.5,-300 224.5,-306 218.5,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"126.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">pricepercent ≤ 0.174</text>\n",
       "<text text-anchor=\"start\" x=\"150\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.48</text>\n",
       "<text text-anchor=\"start\" x=\"146\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5</text>\n",
       "<text text-anchor=\"start\" x=\"144\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [3, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"134\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M217.4164,-341.8796C212.6211,-333.2335 207.5178,-324.0322 202.5662,-315.1042\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"205.5931,-313.3455 197.6821,-306.2981 199.4716,-316.7407 205.5931,-313.3455\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#399de5\" stroke=\"#000000\" d=\"M354.5,-298.5C354.5,-298.5 260.5,-298.5 260.5,-298.5 254.5,-298.5 248.5,-292.5 248.5,-286.5 248.5,-286.5 248.5,-242.5 248.5,-242.5 248.5,-236.5 254.5,-230.5 260.5,-230.5 260.5,-230.5 354.5,-230.5 354.5,-230.5 360.5,-230.5 366.5,-236.5 366.5,-242.5 366.5,-242.5 366.5,-286.5 366.5,-286.5 366.5,-292.5 360.5,-298.5 354.5,-298.5\"/>\n",
       "<text text-anchor=\"start\" x=\"286\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"279\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"277\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"256.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = non&#45;chocolate</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>5&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M263.9333,-341.8796C270.1863,-330.7735 276.9552,-318.7513 283.251,-307.5691\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"286.3235,-309.2459 288.1798,-298.8149 280.2239,-305.8116 286.3235,-309.2459\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.498039\" stroke=\"#000000\" d=\"M158.5,-187C158.5,-187 64.5,-187 64.5,-187 58.5,-187 52.5,-181 52.5,-175 52.5,-175 52.5,-116 52.5,-116 52.5,-110 58.5,-104 64.5,-104 64.5,-104 158.5,-104 158.5,-104 164.5,-104 170.5,-110 170.5,-116 170.5,-116 170.5,-175 170.5,-175 170.5,-181 164.5,-187 158.5,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"61.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">sugarpercent ≤ 0.133</text>\n",
       "<text text-anchor=\"start\" x=\"84\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"81\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"60.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = non&#45;chocolate</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M152.4657,-222.8796C147.936,-214.3236 143.1185,-205.2238 138.4382,-196.3833\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"141.4006,-194.4983 133.6284,-187.2981 135.2141,-197.7736 141.4006,-194.4983\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M274,-179.5C274,-179.5 201,-179.5 201,-179.5 195,-179.5 189,-173.5 189,-167.5 189,-167.5 189,-123.5 189,-123.5 189,-117.5 195,-111.5 201,-111.5 201,-111.5 274,-111.5 274,-111.5 280,-111.5 286,-117.5 286,-123.5 286,-123.5 286,-167.5 286,-167.5 286,-173.5 280,-179.5 274,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"216\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"209\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"207\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"197\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>6&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M196.5343,-222.8796C202.3558,-211.8835 208.6527,-199.9893 214.5227,-188.9015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"217.7476,-190.2905 219.3333,-179.8149 211.5611,-187.0152 217.7476,-190.2905\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M85,-68C85,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 85,0 85,0 91,0 97,-6 97,-12 97,-12 97,-56 97,-56 97,-62 91,-68 85,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"27\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"20\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"18\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M88.0411,-103.9815C83.1078,-95.2504 77.8926,-86.0202 72.9248,-77.2281\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.8263,-75.2483 67.8597,-68.2637 69.7319,-78.6918 75.8263,-75.2483\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#399de5\" stroke=\"#000000\" d=\"M221.5,-68C221.5,-68 127.5,-68 127.5,-68 121.5,-68 115.5,-62 115.5,-56 115.5,-56 115.5,-12 115.5,-12 115.5,-6 121.5,0 127.5,0 127.5,0 221.5,0 221.5,0 227.5,0 233.5,-6 233.5,-12 233.5,-12 233.5,-56 233.5,-56 233.5,-62 227.5,-68 221.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"153\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"146\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"144\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"123.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = non&#45;chocolate</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M134.9589,-103.9815C139.8922,-95.2504 145.1074,-86.0202 150.0752,-77.2281\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"153.2681,-78.6918 155.1403,-68.2637 147.1737,-75.2483 153.2681,-78.6918\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M478,-655.5C478,-655.5 405,-655.5 405,-655.5 399,-655.5 393,-649.5 393,-643.5 393,-643.5 393,-599.5 393,-599.5 393,-593.5 399,-587.5 405,-587.5 405,-587.5 478,-587.5 478,-587.5 484,-587.5 490,-593.5 490,-599.5 490,-599.5 490,-643.5 490,-643.5 490,-649.5 484,-655.5 478,-655.5\"/>\n",
       "<text text-anchor=\"start\" x=\"420\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"410\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 19</text>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [19, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"401\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M442.1502,-698.8796C442.0606,-688.2134 441.9639,-676.7021 441.8731,-665.9015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"445.3724,-665.7851 441.7884,-655.8149 438.3726,-665.844 445.3724,-665.7851\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.874510\" stroke=\"#000000\" d=\"M601,-663C601,-663 520,-663 520,-663 514,-663 508,-657 508,-651 508,-651 508,-592 508,-592 508,-586 514,-580 520,-580 520,-580 601,-580 601,-580 607,-580 613,-586 613,-592 613,-592 613,-651 613,-651 613,-657 607,-663 601,-663\"/>\n",
       "<text text-anchor=\"start\" x=\"516\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">winpercent ≤ 0.502</text>\n",
       "<text text-anchor=\"start\" x=\"533\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.198</text>\n",
       "<text text-anchor=\"start\" x=\"532\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"530\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [8, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"520\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>14&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M483.7707,-698.8796C492.8799,-689.6931 502.6108,-679.8798 511.9776,-670.4336\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"514.4973,-672.8634 519.0532,-663.2981 509.5267,-667.9345 514.4973,-672.8634\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<path fill=\"#399de5\" stroke=\"#000000\" d=\"M546.5,-536.5C546.5,-536.5 452.5,-536.5 452.5,-536.5 446.5,-536.5 440.5,-530.5 440.5,-524.5 440.5,-524.5 440.5,-480.5 440.5,-480.5 440.5,-474.5 446.5,-468.5 452.5,-468.5 452.5,-468.5 546.5,-468.5 546.5,-468.5 552.5,-468.5 558.5,-474.5 558.5,-480.5 558.5,-480.5 558.5,-524.5 558.5,-524.5 558.5,-530.5 552.5,-536.5 546.5,-536.5\"/>\n",
       "<text text-anchor=\"start\" x=\"478\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"471\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"469\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"448.5\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = non&#45;chocolate</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M539.1652,-579.8796C533.5285,-568.8835 527.4315,-556.9893 521.7478,-545.9015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"524.7663,-544.1173 517.09,-536.8149 518.5371,-547.3105 524.7663,-544.1173\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M662,-536.5C662,-536.5 589,-536.5 589,-536.5 583,-536.5 577,-530.5 577,-524.5 577,-524.5 577,-480.5 577,-480.5 577,-474.5 583,-468.5 589,-468.5 589,-468.5 662,-468.5 662,-468.5 668,-468.5 674,-474.5 674,-480.5 674,-480.5 674,-524.5 674,-524.5 674,-530.5 668,-536.5 662,-536.5\"/>\n",
       "<text text-anchor=\"start\" x=\"604\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"597\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8</text>\n",
       "<text text-anchor=\"start\" x=\"595\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [8, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"585\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = chocolate</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>16&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M583.2338,-579.8796C589.2401,-568.8835 595.737,-556.9893 601.7933,-545.9015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"605.0345,-547.2689 606.7565,-536.8149 598.8912,-543.9133 605.0345,-547.2689\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1a16a14a58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=labels_dropped_df.columns.values,  \n",
    "                         class_names=['chocolate', 'non-chocolate'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we worked through every stage of the OSEMN process to build and interpret a classifier to determine whether or not a candy was chocolate.  Most of our effort was spent on the **_Explore_**, **_Model_**, and **_iNterpret_** stages.  \n",
    "\n",
    "* During the **_Explore_** stage, we used Exploratory Data Analysis and visualization to help us gain insight about what features might be most useful to help us answer our question.   \n",
    "<br>  \n",
    "* During the **_Model_** stage, we learned about:\n",
    "    * _Confusion Matrices_ and how to calculate _accuracy_ for binary classification.\n",
    "    * _Entropy_ and _Information Gain_, and how Decision Trees decide what to split on. \n",
    "    * How to build and fit Decision Tree Classifiers using sklearn\n",
    "    * `train_test_split()` and _cross-validation_, and how they help us make sure our model isn't _overfitting_ our training data.    \n",
    "<br>    \n",
    "* Finally, during the **_iNterpret_** stage, we learned how to visualize our Decision Tree, to make the model human-readable.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
